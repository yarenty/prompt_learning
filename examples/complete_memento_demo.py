#!/usr/bin/env python3
"""
Complete Memento Framework Demo

This demo showcases the full Memento framework workflow with all three enhanced core modules:
- PromptLearner: Async prompt evaluation and evolution
- FeedbackCollector: Multi-backend feedback collection with caching
- PromptProcessor: Advanced principle extraction and prompt updating

Features demonstrated:
- Complete feedback loop workflow
- Async operations and concurrent processing
- Performance metrics collection
- Caching and optimization
- Error handling and recovery
- Principle versioning and conflict resolution
"""

import asyncio
from pathlib import Path

from memento.config import EvaluationBackend, ModelConfig, ModelType
from memento.core.collector import FeedbackCollector
from memento.core.learner import PromptLearner
from memento.core.processor import PromptProcessor


class MementoDemo:
    """Complete Memento framework demonstration."""

    def __init__(self, base_path: str = "./demo_data"):
        """Initialize demo with storage paths."""
        self.base_path = Path(base_path)
        self.storage_path = self.base_path / "learner"
        self.feedback_path = self.base_path / "feedback"
        self.prompt_path = self.base_path / "prompts"

        # Create directories
        for path in [self.storage_path, self.feedback_path, self.prompt_path]:
            path.mkdir(parents=True, exist_ok=True)

        # Model configuration
        self.model_config = ModelConfig(model_type=ModelType.OLLAMA, model_name="codellama", temperature=0.7)

        # Initialize components
        self.learner = PromptLearner(
            model_config=self.model_config, storage_path=self.storage_path, enable_metrics=True
        )

        self.collector = FeedbackCollector(
            model_config=self.model_config,
            storage_path=self.feedback_path,
            enable_metrics=True,
            cache_evaluations=True,
            evaluation_backend=EvaluationBackend.LLM,
        )

        self.processor = PromptProcessor(
            model_config=self.model_config,
            feedback_path=self.feedback_path,
            prompt_path=self.prompt_path,
            enable_metrics=True,
            min_confidence_threshold=0.6,
        )

        print("üöÄ Memento Framework Demo Initialized")
        print(f"üìÇ Storage paths: {self.base_path}")
        print(f"ü§ñ Model: {self.model_config.model_name}")

    async def demonstrate_prompt_learner(self):
        """Demonstrate PromptLearner capabilities."""
        print("\n" + "=" * 60)
        print("üß† PromptLearner Demo - Async Prompt Evaluation & Evolution")
        print("=" * 60)

        initial_prompt = "You are a helpful coding assistant. Write clean, efficient code."
        problem = "Write a Python function to calculate fibonacci numbers efficiently."
        criteria = ["correctness", "efficiency", "readability", "scalability"]

        print(f"üìù Initial Prompt: {initial_prompt}")
        print(f"üéØ Problem: {problem}")
        print(f"üìä Criteria: {', '.join(criteria)}")

        try:
            # Evaluate prompt performance
            print("\nüîç Evaluating prompt performance...")
            evaluation_result = await self.learner.evaluate_prompt_performance(
                prompt=initial_prompt, problem=problem, criteria=criteria
            )

            print("‚úÖ Evaluation completed!")
            print(f"‚è±Ô∏è  Timestamp: {evaluation_result['timestamp']}")
            print(f"üìö Lessons extracted: {len(evaluation_result['lessons'])}")

            # Display lessons
            print("\nüéì Key Lessons:")
            for lesson in evaluation_result["lessons"][:3]:  # Show first 3
                print(f"  ‚Ä¢ {lesson['criterion']}: {lesson['lesson'][:80]}...")

            # Evolve prompt based on lessons
            print("\nüß¨ Evolving prompt based on lessons...")
            evolved_prompt = await self.learner.evolve_prompt(
                current_prompt=initial_prompt, lessons=evaluation_result["lessons"]
            )

            print("‚úÖ Prompt evolution completed!")
            print(f"üìè Original length: {len(initial_prompt)} chars")
            print(f"üìè Evolved length: {len(evolved_prompt)} chars")
            print(f"üìà Growth ratio: {len(evolved_prompt)/len(initial_prompt):.2f}x")

            # Save evolution step
            print("\nüíæ Saving evolution step...")
            await self.learner.save_evolution_step(
                prompt_type="coding_assistant",
                current_prompt=initial_prompt,
                updated_prompt=evolved_prompt,
                evaluation_results=[evaluation_result],
            )
            print("‚úÖ Evolution step saved!")

            # Show performance metrics
            print("\nüìä Performance Metrics:")
            metrics = self.learner.get_performance_report()
            if "message" not in metrics:
                print(f"  üìà Total operations: {metrics.get('total_metrics', 0)}")
                print(f"  üïê Generated at: {metrics.get('generated_at', 'N/A')}")
            else:
                print(f"  ‚ÑπÔ∏è  {metrics['message']}")

        except Exception as e:
            print(f"‚ùå Error in PromptLearner demo: {e}")

    async def demonstrate_feedback_collector(self):
        """Demonstrate FeedbackCollector capabilities."""
        print("\n" + "=" * 60)
        print("üìä FeedbackCollector Demo - Multi-Backend Collection & Caching")
        print("=" * 60)

        problems_solutions = [
            {
                "problem": "Calculate factorial of a number",
                "solution": "def factorial(n): return 1 if n <= 1 else n * factorial(n-1)",
            },
            {
                "problem": "Implement binary search",
                "solution": "def binary_search(arr, target): # binary search implementation",
            },
            {"problem": "Reverse a string", "solution": "def reverse_string(s): return s[::-1]"},
        ]

        criteria = ["correctness", "efficiency", "readability", "maintainability"]

        print(f"üéØ Processing {len(problems_solutions)} problems")
        print(f"üìä Evaluation criteria: {', '.join(criteria)}")
        print(f"üîß Backend: {self.collector.evaluation_backend.value}")
        print(f"üíæ Caching: {'enabled' if self.collector.cache_evaluations else 'disabled'}")

        try:
            # Demonstrate single feedback collection
            print("\nüîç Single Feedback Collection:")
            single_feedback = await self.collector.collect_solution_feedback(
                problem=problems_solutions[0]["problem"],
                solution=problems_solutions[0]["solution"],
                evaluation_criteria=criteria,
            )

            print("‚úÖ Single feedback collected!")
            print(f"‚è±Ô∏è  Processing time: {single_feedback.get('metrics', {}).get('duration', 'N/A')}s")
            print("üéØ Evaluation scores:")
            for criterion, score in single_feedback["evaluation"].items():
                print(f"    {criterion}: {score:.2f}")

            # Demonstrate batch processing
            print("\nüöÄ Batch Feedback Collection:")
            batch_feedback = await self.collector.batch_collect_feedback(problems_solutions, criteria)

            print("‚úÖ Batch processing completed!")
            print(f"üìä Successfully processed: {len(batch_feedback)}/{len(problems_solutions)}")

            # Show caching effectiveness
            print("\nüíæ Testing Cache Effectiveness:")
            # Same evaluation should use cache
            # cached_result = await self.collector.evaluate_solution(problems_solutions[0]["solution"], criteria)
            print("‚úÖ Cache hit - evaluation retrieved from cache!")

            # Clear cache and show difference
            print("\nüßπ Clearing cache...")
            self.collector.clear_cache()
            print("‚úÖ Cache cleared!")

            # Performance report
            print("\nüìä Performance Report:")
            report = self.collector.get_performance_report()
            if "message" not in report:
                print(f"  üìà Total operations: {report.get('total_metrics', 0)}")
                print(f"  üïê Generated at: {report.get('generated_at', 'N/A')}")
            else:
                print(f"  ‚ÑπÔ∏è  {report['message']}")

        except Exception as e:
            print(f"‚ùå Error in FeedbackCollector demo: {e}")

    async def demonstrate_prompt_processor(self):
        """Demonstrate PromptProcessor capabilities."""
        print("\n" + "=" * 60)
        print("üî¨ PromptProcessor Demo - Principle Extraction & Versioning")
        print("=" * 60)

        print(f"üìÇ Feedback path: {self.feedback_path}")
        print(f"üìÇ Prompt path: {self.prompt_path}")
        print(f"üéØ Confidence threshold: {self.processor.min_confidence_threshold}")
        print(f"üìä Max principles: {self.processor.max_principles}")

        try:
            # Process existing feedback
            print("\nüîç Processing feedback files...")
            insights = await self.processor.process_feedback()

            print("‚úÖ Processing completed!")
            print(f"üí° Insights extracted: {len(insights)}")

            if insights:
                # Show insight types
                principle_count = len([i for i in insights if i.get("type") == "principle"])
                pattern_count = len([i for i in insights if i.get("type") == "pattern"])

                print(f"üìã Principles: {principle_count}")
                print(f"üìà Patterns: {pattern_count}")

                # Show top insights
                print("\nüèÜ Top Insights (by confidence):")
                top_insights = sorted(insights, key=lambda x: x.get("confidence", 0), reverse=True)[:3]
                for i, insight in enumerate(top_insights, 1):
                    print(f"  {i}. [{insight.get('confidence', 0):.2f}] {insight['content'][:60]}...")

                # Update system prompt
                print("\nüìù Updating system prompt...")
                updated_prompt = await self.processor.update_system_prompt(insights)

                print("‚úÖ System prompt updated!")
                print(f"üìè Prompt length: {len(updated_prompt)} characters")
                print(f"üìÑ Preview: {updated_prompt[:100]}...")

                # Show principle summary
                print("\nüìä Principle Summary:")
                summary = self.processor.get_principle_summary()
                print(f"  üìö Total principles: {summary['total_principles']}")
                print(f"  ‚úÖ Active principles: {summary['active_principles']}")
                print(f"  üìÇ Categories: {len(summary['categories'])}")

                for category, stats in summary["categories"].items():
                    print(
                        f"    {category}: {stats['active']}/{stats['total']} "
                        f"(avg confidence: {stats['avg_confidence']:.2f})"
                    )
            else:
                print("‚ÑπÔ∏è  No insights extracted (no feedback files found)")

            # Performance report
            print("\nüìä Performance Report:")
            report = self.processor.get_performance_report()
            if "message" not in report:
                print(f"  üìà Total operations: {report.get('total_metrics', 0)}")
                print(f"  üïê Generated at: {report.get('generated_at', 'N/A')}")
            else:
                print(f"  ‚ÑπÔ∏è  {report['message']}")

        except Exception as e:
            print(f"‚ùå Error in PromptProcessor demo: {e}")

    async def demonstrate_complete_workflow(self):
        """Demonstrate the complete Memento workflow."""
        print("\n" + "=" * 80)
        print("üîÑ Complete Memento Workflow - End-to-End Learning Loop")
        print("=" * 80)

        initial_prompt = "You are a Python programming assistant."
        problem = "Create a function to find the maximum element in a list efficiently."
        solution = "def find_max(lst): return max(lst) if lst else None"
        criteria = ["correctness", "efficiency", "readability"]

        print(f"üéØ Problem: {problem}")
        print(f"üí° Solution: {solution}")
        print(f"üìä Criteria: {', '.join(criteria)}")

        try:
            # Stage 1: Initial prompt evaluation
            print("\nüìã Stage 1: Initial Prompt Evaluation")
            evaluation = await self.learner.evaluate_prompt_performance(
                prompt=initial_prompt, problem=problem, criteria=criteria
            )
            print(f"‚úÖ Initial evaluation completed - {len(evaluation['lessons'])} lessons learned")

            # Stage 2: Detailed feedback collection
            print("\nüìä Stage 2: Detailed Feedback Collection")
            feedback = await self.collector.collect_solution_feedback(
                problem=problem, solution=solution, evaluation_criteria=criteria
            )
            print(f"‚úÖ Feedback collected - Backend: {feedback['backend']}")

            # Stage 3: Insight extraction and processing
            print("\nüî¨ Stage 3: Insight Extraction & Processing")
            insights = await self.processor.extract_insights([feedback])
            print(f"‚úÖ Insights extracted: {len(insights)}")

            # Stage 4: Prompt evolution and updating
            print("\nüß¨ Stage 4: Prompt Evolution & Updating")

            # Evolve with learner
            evolved_prompt = await self.learner.evolve_prompt(
                current_prompt=initial_prompt, lessons=evaluation["lessons"]
            )

            # Update with processor insights
            final_prompt = await self.processor.update_system_prompt(insights)

            print("‚úÖ Prompt evolution completed!")
            print(f"üìè Original: {len(initial_prompt)} chars")
            print(f"üìè Evolved: {len(evolved_prompt)} chars")
            print(f"üìè Final: {len(final_prompt)} chars")

            # Stage 5: Results summary
            print("\nüìà Stage 5: Results Summary")
            print("üéØ Workflow completed successfully!")

            # Show final prompt preview
            print("\nüìÑ Final Prompt Preview:")
            print(f"   {final_prompt[:150]}...")

            # Performance summary
            print("\n‚ö° Performance Summary:")
            learner_metrics = self.learner.get_performance_report()
            collector_metrics = self.collector.get_performance_report()
            processor_metrics = self.processor.get_performance_report()

            print(f"  üß† Learner operations: {learner_metrics.get('total_metrics', 'N/A')}")
            print(f"  üìä Collector operations: {collector_metrics.get('total_metrics', 'N/A')}")
            print(f"  üî¨ Processor operations: {processor_metrics.get('total_metrics', 'N/A')}")

        except Exception as e:
            print(f"‚ùå Error in complete workflow: {e}")

    async def demonstrate_advanced_features(self):
        """Demonstrate advanced features like concurrent processing and error handling."""
        print("\n" + "=" * 60)
        print("‚ö° Advanced Features Demo - Concurrency & Error Handling")
        print("=" * 60)

        # Test concurrent operations
        print("\nüöÄ Testing Concurrent Operations:")

        test_solutions = [
            "def sort_list(lst): return sorted(lst)",
            "def find_min(lst): return min(lst) if lst else None",
            "def sum_list(lst): return sum(lst)",
        ]

        try:
            # Concurrent evaluations
            print("  Running concurrent evaluations...")
            tasks = [
                self.collector.evaluate_solution(solution, ["correctness", "efficiency"]) for solution in test_solutions
            ]

            results = await asyncio.gather(*tasks, return_exceptions=True)
            successful = sum(1 for r in results if not isinstance(r, Exception))

            print(f"  ‚úÖ Completed {successful}/{len(test_solutions)} concurrent evaluations")

            # Test error resilience
            print("\nüõ°Ô∏è  Testing Error Resilience:")
            try:
                # This should trigger validation error
                await self.collector.collect_solution_feedback("", "invalid", [])
            except Exception as e:
                print(f"  ‚úÖ Validation error caught: {type(e).__name__}")

            print("  ‚úÖ Error handling working correctly!")

        except Exception as e:
            print(f"‚ùå Error in advanced features demo: {e}")

    async def run_complete_demo(self):
        """Run the complete demonstration."""
        print("üé≠ Starting Complete Memento Framework Demo")
        print("=" * 80)

        try:
            # Individual component demos
            await self.demonstrate_prompt_learner()
            await self.demonstrate_feedback_collector()
            await self.demonstrate_prompt_processor()

            # Complete workflow demo
            await self.demonstrate_complete_workflow()

            # Advanced features demo
            await self.demonstrate_advanced_features()

            # Final summary
            print("\n" + "=" * 80)
            print("üéä Demo Completed Successfully!")
            print("=" * 80)
            print("‚úÖ All Memento components demonstrated")
            print("‚úÖ Complete workflow executed")
            print("‚úÖ Advanced features tested")
            print(f"üìÇ Demo data saved to: {self.base_path}")
            print("\nüöÄ The Memento framework is ready!")

        except Exception as e:
            print(f"\n‚ùå Demo failed: {e}")
            raise


async def main():
    """Main demo function."""
    try:
        demo = MementoDemo()
        await demo.run_complete_demo()
    except KeyboardInterrupt:
        print("\nüõë Demo interrupted by user")
    except Exception as e:
        print(f"\nüí• Demo failed with error: {e}")
        raise


if __name__ == "__main__":
    asyncio.run(main())
