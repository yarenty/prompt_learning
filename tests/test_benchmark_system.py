#!/usr/bin/env python3
"""Test script to verify benchmarking system components."""

import sys
from pathlib import Path


def test_imports():
    """Test that all required modules can be imported."""
    print("ğŸ” Testing imports...")

    try:
        # Test core imports
        from memento.config.models import ModelConfig, ModelType

        print("âœ… ModelConfig and ModelType imported")

        from memento.benchmarking.comprehensive_benchmark import ComprehensiveBenchmark

        print("âœ… ComprehensiveBenchmark imported")

        from memento.benchmarking.main import ComprehensiveBenchmark as MainBenchmark

        print("âœ… Main ComprehensiveBenchmark imported")

        from memento.benchmarking.datasets.loader import DatasetLoader

        print("âœ… DatasetLoader imported")

        from memento.benchmarking.evaluation.model_evaluator import ModelEvaluator

        print("âœ… ModelEvaluator imported")

        from memento.benchmarking.baselines.promptbreeder import PromptBreeder

        print("âœ… PromptBreeder imported")

        from memento.benchmarking.baselines.self_evolving_gpt import SelfEvolvingGPT

        print("âœ… SelfEvolvingGPT imported")

        from memento.benchmarking.baselines.auto_evolve import AutoEvolve

        print("âœ… AutoEvolve imported")

        from memento.benchmarking.evaluation.task_metrics import MathematicsMetrics, ProgrammingMetrics, WritingMetrics

        print("âœ… Task metrics imported")

        return True

    except ImportError as e:
        print(f"âŒ Import failed: {e}")
        return False


def test_model_config():
    """Test model configuration creation."""
    print("\nğŸ” Testing model configuration...")

    try:
        from memento.config.models import ModelConfig, ModelType

        config = ModelConfig(model_type=ModelType.OLLAMA, model_name="llama3.2", temperature=0.7, max_tokens=2048)

        print(f"âœ… ModelConfig created: {config.model_name}")
        return True

    except Exception as e:
        print(f"âŒ ModelConfig creation failed: {e}")
        return False


def test_dataset_loader():
    """Test dataset loader initialization."""
    print("\nğŸ” Testing dataset loader...")

    try:
        from memento.benchmarking.datasets.loader import DatasetLoader

        loader = DatasetLoader()
        available_datasets = list(loader.datasets.keys())

        print(f"âœ… DatasetLoader created with {len(available_datasets)} datasets: {available_datasets}")
        return True

    except Exception as e:
        print(f"âŒ DatasetLoader creation failed: {e}")
        return False


def test_benchmark_creation():
    """Test benchmark system creation."""
    print("\nğŸ” Testing benchmark creation...")

    try:
        from memento.benchmarking.comprehensive_benchmark import ComprehensiveBenchmark
        from memento.config.models import ModelConfig, ModelType

        model_config = ModelConfig(model_type=ModelType.OLLAMA, model_name="llama3.2", temperature=0.7, max_tokens=2048)

        benchmark = ComprehensiveBenchmark(
            model_config=model_config,
            output_dir="test_output",
            enable_dashboard=False,
            enable_resource_monitoring=False,
        )

        print("âœ… ComprehensiveBenchmark created successfully")
        return True

    except Exception as e:
        print(f"âŒ Benchmark creation failed: {e}")
        return False


def test_baseline_models():
    """Test baseline model creation."""
    print("\nğŸ” Testing baseline models...")

    try:
        from memento.benchmarking.baselines.auto_evolve import AutoEvolve
        from memento.benchmarking.baselines.promptbreeder import PromptBreeder
        from memento.benchmarking.baselines.self_evolving_gpt import SelfEvolvingGPT
        from memento.config.models import ModelConfig, ModelType

        model_config = ModelConfig(model_type=ModelType.OLLAMA, model_name="llama3.2", temperature=0.7, max_tokens=2048)

        # Test PromptBreeder
        promptbreeder = PromptBreeder(model_config=model_config, storage_path=Path("test_promptbreeder"))
        print("âœ… PromptBreeder created")

        # Test SelfEvolvingGPT
        self_evolving = SelfEvolvingGPT(model_config=model_config, storage_path=Path("test_self_evolving"))
        print("âœ… SelfEvolvingGPT created")

        # Test AutoEvolve
        auto_evolve = AutoEvolve(model_config=model_config, storage_path=Path("test_auto_evolve"))
        print("âœ… AutoEvolve created")

        return True

    except Exception as e:
        print(f"âŒ Baseline model creation failed: {e}")
        return False


def test_task_metrics():
    """Test task metrics creation."""
    print("\nğŸ” Testing task metrics...")

    try:
        from memento.benchmarking.evaluation.task_metrics import MathematicsMetrics, ProgrammingMetrics, WritingMetrics

        programming_metrics = ProgrammingMetrics()
        print("âœ… ProgrammingMetrics created")

        math_metrics = MathematicsMetrics()
        print("âœ… MathematicsMetrics created")

        writing_metrics = WritingMetrics()
        print("âœ… WritingMetrics created")

        return True

    except Exception as e:
        print(f"âŒ Task metrics creation failed: {e}")
        return False


def main():
    """Run all tests."""
    print("ğŸš€ Testing Memento Benchmarking System")
    print("=" * 50)

    tests = [
        test_imports,
        test_model_config,
        test_dataset_loader,
        test_benchmark_creation,
        test_baseline_models,
        test_task_metrics,
    ]

    passed = 0
    total = len(tests)

    for test in tests:
        if test():
            passed += 1

    print("\n" + "=" * 50)
    print(f"ğŸ“Š Test Results: {passed}/{total} tests passed")

    if passed == total:
        print("ğŸ‰ All tests passed! The benchmarking system is ready to use.")
        print("\nğŸ’¡ You can now run benchmarks with:")
        print("   python -m memento.cli.benchmark run --datasets humaneval --models memento --max-samples 5")
    else:
        print("âš ï¸  Some tests failed. Please check the error messages above.")
        sys.exit(1)


if __name__ == "__main__":
    main()
