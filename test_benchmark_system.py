#!/usr/bin/env python3
"""Test script to verify benchmarking system components."""

import sys
from pathlib import Path

def test_imports():
    """Test that all required modules can be imported."""
    print("üîç Testing imports...")
    
    try:
        # Test core imports
        from memento.config.models import ModelConfig, ModelType
        print("‚úÖ ModelConfig and ModelType imported")
        
        from memento.benchmarking.comprehensive_benchmark import ComprehensiveBenchmark
        print("‚úÖ ComprehensiveBenchmark imported")
        
        from memento.benchmarking.main import ComprehensiveBenchmark as MainBenchmark
        print("‚úÖ Main ComprehensiveBenchmark imported")
        
        from memento.benchmarking.datasets.loader import DatasetLoader
        print("‚úÖ DatasetLoader imported")
        
        from memento.benchmarking.evaluation.model_evaluator import ModelEvaluator
        print("‚úÖ ModelEvaluator imported")
        
        from memento.benchmarking.baselines.promptbreeder import PromptBreeder
        print("‚úÖ PromptBreeder imported")
        
        from memento.benchmarking.baselines.self_evolving_gpt import SelfEvolvingGPT
        print("‚úÖ SelfEvolvingGPT imported")
        
        from memento.benchmarking.baselines.auto_evolve import AutoEvolve
        print("‚úÖ AutoEvolve imported")
        
        from memento.benchmarking.evaluation.task_metrics import ProgrammingMetrics, MathematicsMetrics, WritingMetrics
        print("‚úÖ Task metrics imported")
        
        return True
        
    except ImportError as e:
        print(f"‚ùå Import failed: {e}")
        return False

def test_model_config():
    """Test model configuration creation."""
    print("\nüîç Testing model configuration...")
    
    try:
        from memento.config.models import ModelConfig, ModelType
        
        config = ModelConfig(
            model_type=ModelType.OLLAMA,
            model_name="llama3.2",
            temperature=0.7,
            max_tokens=2048
        )
        
        print(f"‚úÖ ModelConfig created: {config.model_name}")
        return True
        
    except Exception as e:
        print(f"‚ùå ModelConfig creation failed: {e}")
        return False

def test_dataset_loader():
    """Test dataset loader initialization."""
    print("\nüîç Testing dataset loader...")
    
    try:
        from memento.benchmarking.datasets.loader import DatasetLoader
        
        loader = DatasetLoader()
        available_datasets = list(loader.datasets.keys())
        
        print(f"‚úÖ DatasetLoader created with {len(available_datasets)} datasets: {available_datasets}")
        return True
        
    except Exception as e:
        print(f"‚ùå DatasetLoader creation failed: {e}")
        return False

def test_benchmark_creation():
    """Test benchmark system creation."""
    print("\nüîç Testing benchmark creation...")
    
    try:
        from memento.config.models import ModelConfig, ModelType
        from memento.benchmarking.comprehensive_benchmark import ComprehensiveBenchmark
        
        model_config = ModelConfig(
            model_type=ModelType.OLLAMA,
            model_name="llama3.2",
            temperature=0.7,
            max_tokens=2048
        )
        
        benchmark = ComprehensiveBenchmark(
            model_config=model_config,
            output_dir="test_output",
            enable_dashboard=False,
            enable_resource_monitoring=False
        )
        
        print("‚úÖ ComprehensiveBenchmark created successfully")
        return True
        
    except Exception as e:
        print(f"‚ùå Benchmark creation failed: {e}")
        return False

def test_baseline_models():
    """Test baseline model creation."""
    print("\nüîç Testing baseline models...")
    
    try:
        from memento.config.models import ModelConfig, ModelType
        from memento.benchmarking.baselines.promptbreeder import PromptBreeder
        from memento.benchmarking.baselines.self_evolving_gpt import SelfEvolvingGPT
        from memento.benchmarking.baselines.auto_evolve import AutoEvolve
        
        model_config = ModelConfig(
            model_type=ModelType.OLLAMA,
            model_name="llama3.2",
            temperature=0.7,
            max_tokens=2048
        )
        
        # Test PromptBreeder
        promptbreeder = PromptBreeder(
            model_config=model_config,
            storage_path=Path("test_promptbreeder")
        )
        print("‚úÖ PromptBreeder created")
        
        # Test SelfEvolvingGPT
        self_evolving = SelfEvolvingGPT(
            model_config=model_config,
            storage_path=Path("test_self_evolving")
        )
        print("‚úÖ SelfEvolvingGPT created")
        
        # Test AutoEvolve
        auto_evolve = AutoEvolve(
            model_config=model_config,
            storage_path=Path("test_auto_evolve")
        )
        print("‚úÖ AutoEvolve created")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Baseline model creation failed: {e}")
        return False

def test_task_metrics():
    """Test task metrics creation."""
    print("\nüîç Testing task metrics...")
    
    try:
        from memento.benchmarking.evaluation.task_metrics import ProgrammingMetrics, MathematicsMetrics, WritingMetrics
        
        programming_metrics = ProgrammingMetrics()
        print("‚úÖ ProgrammingMetrics created")
        
        math_metrics = MathematicsMetrics()
        print("‚úÖ MathematicsMetrics created")
        
        writing_metrics = WritingMetrics()
        print("‚úÖ WritingMetrics created")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Task metrics creation failed: {e}")
        return False

def main():
    """Run all tests."""
    print("üöÄ Testing Memento Benchmarking System")
    print("=" * 50)
    
    tests = [
        test_imports,
        test_model_config,
        test_dataset_loader,
        test_benchmark_creation,
        test_baseline_models,
        test_task_metrics,
    ]
    
    passed = 0
    total = len(tests)
    
    for test in tests:
        if test():
            passed += 1
    
    print("\n" + "=" * 50)
    print(f"üìä Test Results: {passed}/{total} tests passed")
    
    if passed == total:
        print("üéâ All tests passed! The benchmarking system is ready to use.")
        print("\nüí° You can now run benchmarks with:")
        print("   python -m memento.cli.benchmark run --datasets humaneval --models memento --max-samples 5")
    else:
        print("‚ö†Ô∏è  Some tests failed. Please check the error messages above.")
        sys.exit(1)

if __name__ == "__main__":
    main()
